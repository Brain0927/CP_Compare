"""
åœ–åƒè­˜åˆ¥æ¨¡çµ„ - ä½¿ç”¨ Gemini Vision API è­˜åˆ¥å•†å“è¦æ ¼åœ–åƒ
"""
import base64
import requests
from io import BytesIO
from PIL import Image
from typing import Dict, List, Optional
import google.generativeai as genai
from config.settings import GEMINI_API_KEY
import time


class ImageRecognizer:
    """åœ–åƒè­˜åˆ¥å™¨ - ä½¿ç”¨ Gemini Vision è­˜åˆ¥è¦æ ¼åœ–åƒ"""
    
    def __init__(self):
        if GEMINI_API_KEY:
            genai.configure(api_key=GEMINI_API_KEY)
        self.model = genai.GenerativeModel('gemini-1.5-flash')
    
    @staticmethod
    def download_image(url: str) -> Optional[bytes]:
        """ä¸‹è¼‰åœ–åƒç‚ºä½å…ƒçµ„"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, timeout=15, headers=headers)
            if response.status_code == 200:
                print(f"âœ… åœ–åƒä¸‹è¼‰æˆåŠŸ: {url[:60]}...")
                return response.content
            else:
                print(f"âŒ åœ–åƒä¸‹è¼‰å¤±æ•— (HTTP {response.status_code}): {url}")
        except Exception as e:
            print(f"âŒ åœ–åƒä¸‹è¼‰å¤±æ•—: {e} ({url})")
        return None
    
    def extract_specs_from_image_url(self, image_url: str) -> Dict[str, str]:
        """å¾åœ–åƒ URL ä¸­æå–è¦æ ¼è³‡è¨Š"""
        try:
            # ä¸‹è¼‰åœ–åƒ
            image_data = self.download_image(image_url)
            if not image_data:
                return {}
            
            print(f"ğŸ–¼ï¸ æ­£åœ¨ç”¨ Gemini Vision è­˜åˆ¥åœ–åƒè¦æ ¼...")
            
            # ä½¿ç”¨ Gemini Vision è­˜åˆ¥åœ–åƒä¸­çš„è¦æ ¼
            prompt = """ä½ æ˜¯å•†å“è¦æ ¼è­˜åˆ¥å°ˆå®¶ã€‚è«‹åˆ†æé€™å¼µåœ–åƒä¸­çš„å•†å“è¦æ ¼ä¿¡æ¯ã€‚

**è«‹å‹™å¿…æå–ä»¥ä¸‹ä¿¡æ¯ï¼ˆå¦‚æœåœ–åƒä¸­æœ‰çš„è©±ï¼‰ï¼š**
1. æè³ª/ææ–™
2. å°ºå¯¸/å¤§å°/é•·å¯¬é«˜
3. é‡é‡
4. é¡è‰²
5. åŠŸèƒ½/ç‰¹æ€§
6. å‹è™Ÿ
7. ä¿ä¿®/ä¿å›ºæœŸé™
8. é›»æº/é›»æ± 
9. è¦æ ¼/åƒæ•¸
10. å…¶ä»–é‡è¦è¦æ ¼

**è¿”å›æ ¼å¼è¦æ±‚ï¼š**
- æ¯è¡Œä¸€å€‹è¦æ ¼ï¼Œæ ¼å¼ç‚ºã€Œè¦æ ¼åç¨±: è¦æ ¼å€¼ã€
- åªè¿”å›è¦æ ¼ä¿¡æ¯ï¼Œä¸éœ€è¦å…¶ä»–èªªæ˜
- ç›¡å¯èƒ½è©³ç´°æº–ç¢º"""
            
            # å°‡åœ–åƒç™¼é€çµ¦ Gemini Vision
            response = self.model.generate_content([
                prompt,
                Image.open(BytesIO(image_data))
            ])
            
            print(f"ğŸ“ Gemini å“åº”å†…å®¹: {response.text[:100]}...")
            
            # è§£æéŸ¿æ‡‰
            specs = self._parse_specs_response(response.text)
            print(f"âœ… æˆåŠŸè­˜åˆ¥åˆ° {len(specs)} å€‹è¦æ ¼")
            
            return specs
            
        except Exception as e:
            print(f"âŒ åœ–åƒè­˜åˆ¥å¤±æ•—: {str(e)}")
            import traceback
            traceback.print_exc()
            return {}
    
    def extract_specs_from_images(self, image_urls: List[str]) -> Dict[str, str]:
        """å¾å¤šå¼µåœ–åƒä¸­æå–è¦æ ¼è³‡è¨Šï¼ˆåˆä½µæ‰€æœ‰è¦æ ¼ï¼‰"""
        all_specs = {}
        
        print(f"ğŸ“Š é–‹å§‹è­˜åˆ¥ {len(image_urls)} å¼µåœ–åƒ...")
        
        for i, url in enumerate(image_urls, 1):
            print(f"\nã€åœ–åƒ {i}/{len(image_urls)}ã€‘ {url[:50]}...")
            specs = self.extract_specs_from_image_url(url)
            
            if specs:
                all_specs.update(specs)
                print(f"  â†’ æœ¬å¼µåœ–åƒè­˜åˆ¥çµæœ: {len(specs)} å€‹è¦æ ¼")
            else:
                print(f"  â†’ æœ¬å¼µåœ–åƒæœªè­˜åˆ¥åˆ°è¦æ ¼")
            
            # é¿å… API é™æµ
            if i < len(image_urls):
                print("â³ ç­‰å¾…ä¸­... (é¿å… API é™æµ)")
                time.sleep(2)
        
        print(f"\nğŸ“Š æ‰€æœ‰åœ–åƒè­˜åˆ¥å®Œæˆï¼Œå…± {len(all_specs)} å€‹è¦æ ¼")
        return all_specs
    
    @staticmethod
    def _parse_specs_response(response_text: str) -> Dict[str, str]:
        """è§£æ Gemini çš„è¦æ ¼è­˜åˆ¥éŸ¿æ‡‰"""
        specs = {}
        
        print(f"ğŸ“ æ­£åœ¨è§£æ Gemini éŸ¿æ‡‰...")
        lines = response_text.split('\n')
        valid_count = 0
        
        for line in lines:
            line = line.strip()
            if not line or ':' not in line:
                continue
            
            try:
                parts = line.split(':', 1)
                if len(parts) == 2:
                    key = parts[0].strip()
                    value = parts[1].strip()
                    
                    # æ¸…ç† keyï¼ˆç§»é™¤ç·¨è™Ÿã€æ‹¬è™Ÿç­‰ï¼‰
                    key = key.lstrip('0123456789.ï¼‰)ã€ ')
                    
                    # éæ¿¾æ‰ç„¡æ•ˆå€¼
                    if (key and value and 
                        len(key) > 1 and len(value) > 1 and
                        value.lower() not in ['', 'n/a', 'ç„¡', 'æœªæ‰¾åˆ°', 'not found', 'æš«ç„¡']):
                        specs[key] = value
                        valid_count += 1
                        print(f"  âœ“ {key}: {value}")
            except Exception as e:
                continue
        
        print(f"ğŸ“Š æœ¬æ¬¡è­˜åˆ¥çµæœ: {valid_count} å€‹æœ‰æ•ˆè¦æ ¼")
        return specs


class MomoImageExtractor:
    """MOMO ç‰¹å®šçš„åœ–åƒæå–å™¨"""
    
    @staticmethod
    def extract_spec_images_from_soup(soup) -> List[str]:
        """å¾ BeautifulSoup å°è±¡ä¸­æå–è¦æ ¼åœ–åƒ URLï¼ˆMOMO ç‰¹å®šï¼‰"""
        image_urls = []
        
        try:
            print("ğŸ” æ­£åœ¨æœå°‹ MOMO è¦æ ¼åœ–åƒ...")
            
            # æ–¹æ³• 1ï¼šæŸ¥æ‰¾è¦æ ¼åœ–åƒå®¹å™¨
            spec_section = soup.find('div', {'class': lambda x: x and 'spec' in x.lower()})
            
            if spec_section:
                images = spec_section.find_all('img')
                for img in images:
                    src = img.get('src', '') or img.get('data-src', '')
                    alt = img.get('alt', '')
                    
                    # æª¢æŸ¥æ˜¯å¦æ˜¯è¦æ ¼ç›¸é—œåœ–åƒ
                    if src and ('spec' in alt.lower() or 'spec' in src.lower() or 'momo' in src.lower()):
                        if src.startswith('http') or src.startswith('//'):
                            image_urls.append(src if src.startswith('http') else 'https:' + src)
                            print(f"  âœ“ æ‰¾åˆ°è¦æ ¼åœ–åƒ: {src[:60]}...")
            
            # æ–¹æ³• 2ï¼šæŸ¥æ‰¾ç”¢å“è©³æƒ…å€çš„æ‰€æœ‰åœ–åƒ
            if not image_urls:
                print("  âš ï¸ è¦æ ¼å®¹å™¨æœå°‹æœªæœï¼Œå˜—è©¦å‚™é¸æ–¹æ¡ˆ...")
                detail_sections = soup.find_all('div', {'class': lambda x: x and any(
                    kw in x.lower() for kw in ['detail', 'info', 'spec', 'product']
                )})
                
                for section in detail_sections[:3]:  # æœ€å¤šæŸ¥æ‰¾ 3 å€‹å€å¡Š
                    images = section.find_all('img', limit=5)
                    for img in images:
                        src = img.get('src', '') or img.get('data-src', '')
                        if src and ('momo' in src.lower() or '.jpg' in src.lower() or '.png' in src.lower()):
                            if src.startswith('http') or src.startswith('//'):
                                full_url = src if src.startswith('http') else 'https:' + src
                                if full_url not in image_urls:
                                    image_urls.append(full_url)
                                    print(f"  âœ“ æ‰¾åˆ°åœ–åƒ: {src[:60]}...")
            
            # æ–¹æ³• 3ï¼šæŸ¥æ‰¾æ‰€æœ‰æœ‰æ•ˆçš„ç”¢å“åœ–åƒ
            if not image_urls:
                print("  âš ï¸ å˜—è©¦æœ€å¾Œå‚™é¸æ–¹æ¡ˆ...")
                all_imgs = soup.find_all('img', limit=20)
                for img in all_imgs:
                    src = img.get('src', '') or img.get('data-src', '')
                    if src and 'momo' in src.lower() and '.jpg' in src.lower():
                        full_url = src if src.startswith('http') else 'https:' + src
                        if full_url not in image_urls:
                            image_urls.append(full_url)
                            print(f"  âœ“ æ‰¾åˆ°åœ–åƒ: {src[:60]}...")
                            if len(image_urls) >= 5:
                                break
            
            print(f"âœ… å…±æ‰¾åˆ° {len(image_urls)} å¼µåœ–åƒ")
            return image_urls[:5]  # é™åˆ¶æœ€å¤š 5 å¼µåœ–åƒ
        
        except Exception as e:
            print(f"âŒ æå–è¦æ ¼åœ–åƒå¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
        
        return image_urls


def extract_momo_specs_from_images(soup) -> Dict[str, str]:
    """å¾ MOMO é é¢è¦æ ¼åœ–åƒä¸­æå–è³‡è¨Šï¼ˆä¸»å‡½æ•¸ï¼‰"""
    try:
        print("\n" + "="*60)
        print("ğŸ–¼ï¸  é–‹å§‹ MOMO å•†å“è¦æ ¼åœ–åƒè­˜åˆ¥æµç¨‹")
        print("="*60)
        
        # æå–åœ–åƒ URL
        extractor = MomoImageExtractor()
        image_urls = extractor.extract_spec_images_from_soup(soup)
        
        if not image_urls:
            print("âš ï¸  æœªæ‰¾åˆ°è¦æ ¼åœ–åƒ")
            return {}
        
        print(f"\nğŸ“ æ‰¾åˆ° {len(image_urls)} å¼µè¦æ ¼åœ–åƒï¼Œé–‹å§‹è­˜åˆ¥...")
        
        # ä½¿ç”¨åœ–åƒè­˜åˆ¥å™¨æå–è¦æ ¼
        recognizer = ImageRecognizer()
        specs = recognizer.extract_specs_from_images(image_urls)
        
        print("\n" + "="*60)
        print(f"âœ… MOMO è¦æ ¼åœ–åƒè­˜åˆ¥å®Œæˆï¼Œå…±è­˜åˆ¥åˆ° {len(specs)} å€‹è¦æ ¼")
        print("="*60 + "\n")
        
        return specs
        
    except Exception as e:
        print(f"\nâŒ MOMO è¦æ ¼åœ–åƒæå–å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return {}
