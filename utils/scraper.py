"""
çˆ¬èŸ²æ¨¡çµ„ - ä½¿ç”¨ BeautifulSoup èˆ‡ Selenium
"""
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from config.settings import HEADERS, REQUEST_TIMEOUT, SELENIUM_WAIT_TIME
import time
import json

# å°å…¥åœ–åƒè­˜åˆ¥æ¨¡çµ„
try:
    from utils.image_recognizer import extract_momo_specs_from_images
    IMAGE_RECOGNITION_AVAILABLE = True
except ImportError:
    IMAGE_RECOGNITION_AVAILABLE = False
    print("âš ï¸  åœ–åƒè­˜åˆ¥åŠŸèƒ½æœªå®‰è£")


class ProductScraper:
    """å•†å“çˆ¬èŸ²åŸºé¡"""
    
    def __init__(self):
        self.headers = HEADERS
        self.timeout = REQUEST_TIMEOUT
    
    def scrape_static(self, url):
        """çˆ¬å–éœæ…‹é é¢ (BeautifulSoup)"""
        try:
            response = requests.get(url, headers=self.headers, timeout=self.timeout)
            response.encoding = 'utf-8'
            return BeautifulSoup(response.content, 'html.parser')
        except Exception as e:
            print(f"âŒ éœæ…‹é é¢çˆ¬å–å¤±æ•—: {e}")
            return None
    
    def scrape_dynamic(self, url):
        """çˆ¬å–å‹•æ…‹é é¢ (Selenium)"""
        driver = None
        try:
            options = webdriver.ChromeOptions()
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--start-maximized')
            options.add_argument('--disable-blink-features=AutomationControlled')
            options.add_experimental_option("excludeSwitches", ["enable-automation"])
            options.add_experimental_option('useAutomationExtension', False)
            
            driver = webdriver.Chrome(options=options)
            
            # æ·»åŠ  User-Agent
            driver.execute_cdp_cmd('Network.setUserAgentOverride', {
                "userAgent": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            })
            
            driver.get(url)
            
            # ç­‰å¾…é é¢åŠ è¼‰
            WebDriverWait(driver, SELENIUM_WAIT_TIME).until(
                EC.presence_of_all_elements_located((By.TAG_NAME, "body"))
            )
            
            time.sleep(3)  # é¡å¤–ç­‰å¾…æ™‚é–“ç¢ºä¿ JS åŸ·è¡Œå®Œæˆ
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            return soup
            
        except Exception as e:
            print(f"âŒ å‹•æ…‹é é¢çˆ¬å–å¤±æ•—: {e}")
            return None
        finally:
            if driver:
                driver.quit()
    
    def extract_product_info(self, url, is_dynamic=False):
        """
        æå–å•†å“è³‡è¨Š
        
        Args:
            url: å•†å“é€£çµ
            is_dynamic: æ˜¯å¦ç‚ºå‹•æ…‹é é¢
        
        Returns:
            dict: å•†å“è³‡è¨Š {name, price, specs, reviews, url}
        """
        # Momo å•†å“è‡ªå‹•ä½¿ç”¨å‹•æ…‹çˆ¬å–ï¼ˆå› ç‚ºåƒ¹æ ¼é€šé JS è¼‰å…¥ï¼‰
        if 'momo.com.tw' in url.lower():
            is_dynamic = True
        
        # å…ˆå˜—è©¦å‹•æ…‹çˆ¬å–
        soup = None
        if is_dynamic:
            soup = self.scrape_dynamic(url)
        
        # å¦‚æœå‹•æ…‹çˆ¬å–å¤±æ•—ï¼Œé™ç´šåˆ°éœæ…‹çˆ¬å–
        if not soup:
            print(f"âš ï¸  å‹•æ…‹çˆ¬å–å¤±æ•—ï¼Œé™ç´šåˆ°éœæ…‹çˆ¬å–...")
            soup = self.scrape_static(url)
        
        if not soup:
            return None
        
        # é€šç”¨çˆ¬å–é‚è¼¯ï¼ˆç°¡åŒ–ç‰ˆï¼Œå¯¦éš›éœ€æ ¹æ“šå„å¹³å°èª¿æ•´ï¼‰
        product_info = {
            "url": url,
            "name": self._extract_name(soup),
            "price": self._extract_price(soup),
            "specs": self._extract_specs(soup),
            "reviews": self._extract_reviews(soup),
            "rating": self._extract_rating(soup)
        }
        
        return product_info
    
    def _extract_name(self, soup):
        """æå–å•†å“åç¨±"""
        # Momo ç‰¹å®šé¸æ“‡å™¨
        momo_selectors = [
            'h1.title',
            'h1[class*="title"]',
            'div.goods-name',
            'span.goods-title',
            'div[data-testid*="name"]'
        ]
        
        # é€šç”¨é¸æ“‡å™¨
        general_selectors = ['h1', '.product-title', '[data-name]', '.title', 'h2']
        
        all_selectors = momo_selectors + general_selectors
        
        for selector in all_selectors:
            try:
                elem = soup.select_one(selector)
                if elem:
                    name = elem.get_text(strip=True)
                    if name and len(name) > 3:  # ç¢ºä¿ä¸æ˜¯å¤ªçŸ­çš„æ–‡æœ¬
                        return name
            except:
                continue
        
        return "æœªçŸ¥å•†å“"
    
    def _extract_price(self, soup):
        """æå–åƒ¹æ ¼ - æ”¯æ´å¤šå€‹å¹³å°ï¼Œå„ªå…ˆæå–ä¿ƒéŠ·åƒ¹"""
        
        # === Momo ç‰¹å®šé‚è¼¯ï¼šå„ªå…ˆä½¿ç”¨ä¿ƒéŠ·åƒ¹ï¼Œå¦å‰‡ä½¿ç”¨å¸‚å”®åƒ¹ ===
        # ä¿ƒéŠ·åƒ¹ (ç´…è‰²é¡¯ç¤ºçš„åƒ¹æ ¼)
        promo_price = soup.find('span', class_='seoPrice')
        if promo_price:
            text = promo_price.get_text(strip=True)
            price_str = ''.join(c for c in text if c.isdigit() or c in '.,')
            price_str = price_str.replace(',', '')
            try:
                price = float(price_str)
                if price > 0:
                    return price
            except:
                pass
        
        # å¸‚å”®åƒ¹ (åˆªé™¤ç·šçš„åƒ¹æ ¼)
        sale_price = soup.find('del', class_='seoPrice')
        if sale_price:
            text = sale_price.get_text(strip=True)
            price_str = ''.join(c for c in text if c.isdigit() or c in '.,')
            price_str = price_str.replace(',', '')
            try:
                price = float(price_str)
                if price > 0:
                    return price
            except:
                pass
        
        # === å‚™ç”¨é¸æ“‡å™¨ï¼ˆå…¶ä»–å¹³å°æˆ–çµæ§‹ï¼‰ ===
        momo_selectors = [
            'span.money',                      # Momo ä¸»è¦åƒ¹æ ¼
            'p.current-price span.money',      # Momo å®Œæ•´è·¯å¾‘
            'span[class*="money"]',            # Momo æ¨¡ç³ŠåŒ¹é…
            'div.goods-price',
            'strong.price',
            'em.price',
            'span[class*="salesprice"]',
            'span[class*="sale-price"]',
        ]
        
        # é€šç”¨é¸æ“‡å™¨ï¼ˆå‚™ç”¨ï¼‰
        general_selectors = [
            '.price', '[data-price]', '.product-price', '.sale-price', 
            '.final-price', '.current-price', '.priceText'
        ]
        
        all_selectors = momo_selectors + general_selectors
        
        for selector in all_selectors:
            try:
                elem = soup.select_one(selector)
                if elem:
                    # å˜—è©¦å¾ data å±¬æ€§ä¸­ç²å–
                    if elem.get('data-price'):
                        try:
                            return float(elem.get('data-price'))
                        except:
                            pass
                    
                    # å¾æ–‡æœ¬å…§å®¹ä¸­æå–
                    text = elem.get_text(strip=True)
                    if text:
                        # ç§»é™¤éæ•¸å­—å­—å…ƒï¼ˆä¿ç•™å°æ•¸é»å’Œé€—è™Ÿï¼‰
                        price_str = ''.join(c for c in text if c.isdigit() or c in '.,')
                        
                        # ç§»é™¤é€—è™Ÿï¼ˆåƒä½åˆ†éš”ç¬¦ï¼‰
                        price_str = price_str.replace(',', '')
                        
                        # ç§»é™¤å¤šå€‹å°æ•¸é»ï¼Œåªä¿ç•™æœ€å¾Œä¸€å€‹
                        if price_str.count('.') > 1:
                            parts = price_str.split('.')
                            price_str = '.'.join([parts[0], parts[-1]])
                        
                        if price_str and price_str != '.':
                            try:
                                price = float(price_str)
                                if price > 0:  # ç¢ºä¿åƒ¹æ ¼æœ‰æ•ˆ
                                    return price
                            except ValueError:
                                continue
            except:
                continue
        
        return 0
    
    def _extract_specs(self, soup):
        """æå–è¦æ ¼è³‡è¨Š - æ”¯æ´ Momo çµæ§‹ä¸¦ä½¿ç”¨åœ–åƒè­˜åˆ¥"""
        specs = {}
        
        # === Momo è¦æ ¼è¡¨çµæ§‹ ===
        # å˜—è©¦æ‰¾åˆ°è¦æ ¼å®¹å™¨
        spec_containers = soup.find_all(['dl', 'table', 'div'], class_=lambda x: x and any(
            keyword in x.lower() for keyword in ['spec', 'attribute', 'property', 'info', 'detail', 'è¦æ ¼']
        ))
        
        # å˜—è©¦ DL/DT/DD çµæ§‹
        for container in spec_containers:
            dts = container.find_all('dt') if container.find_all('dt') else []
            dds = container.find_all('dd') if container.find_all('dd') else []
            
            for dt, dd in zip(dts, dds):
                key = dt.get_text(strip=True)
                value = dd.get_text(strip=True)
                if key and value and len(key) < 50 and len(value) < 200:
                    specs[key] = value
        
        # å˜—è©¦è¡¨æ ¼çµæ§‹
        tables = soup.find_all('table')
        for table in tables:
            rows = table.find_all('tr')
            for row in rows:
                cells = row.find_all(['td', 'th'])
                if len(cells) >= 2:
                    key = cells[0].get_text(strip=True)
                    value = cells[1].get_text(strip=True)
                    if key and value and len(key) < 50 and len(value) < 200:
                        specs[key] = value
        
        # å˜—è©¦ Div çµæ§‹ï¼ˆMomo å¸¸ç”¨ï¼‰
        if not specs:
            # å°‹æ‰¾åŒ…å«ã€Œè¦æ ¼ã€æˆ–ã€Œç‰¹æ€§ã€çš„ div
            spec_divs = soup.find_all('div', class_=lambda x: x and any(
                kw in x.lower() for kw in ['spec', 'detail', 'property', 'info']
            ))
            
            for div in spec_divs:
                # å°‹æ‰¾æ¨™ç±¤å’Œæ•¸å€¼å°
                labels = div.find_all(['label', 'strong', 'b'], limit=10)
                for label in labels:
                    label_text = label.get_text(strip=True)
                    # æ‰¾åˆ°ç·Šé„°çš„å€¼
                    next_elem = label.find_next('span', 'div', 'td', 'dd')
                    if next_elem:
                        value_text = next_elem.get_text(strip=True)
                        if label_text and value_text:
                            specs[label_text] = value_text
        
        # === MOMO ç‰¹å®šï¼šä½¿ç”¨åœ–åƒè­˜åˆ¥è£œå……è¦æ ¼ ===
        if IMAGE_RECOGNITION_AVAILABLE:
            print("ğŸ–¼ï¸  å˜—è©¦å¾è¦æ ¼åœ–åƒä¸­æå–è³‡è¨Š...")
            try:
                image_specs = extract_momo_specs_from_images(soup)
                if image_specs:
                    print(f"âœ… å¾åœ–åƒä¸­è­˜åˆ¥åˆ° {len(image_specs)} å€‹è¦æ ¼")
                    specs.update(image_specs)
            except Exception as e:
                print(f"âš ï¸  åœ–åƒè­˜åˆ¥å¤±æ•— (éè‡´å‘½): {e}")
        
        return specs
    
    def _extract_reviews(self, soup):
        """æå–è©•è«–èˆ‡è©•åƒ¹"""
        reviews = []
        review_elements = soup.find_all(['div', 'li'], class_=lambda x: x and 'review' in x.lower())
        
        for elem in review_elements[:5]:  # é™åˆ¶5å‰‡è©•è«–
            review_text = elem.get_text(strip=True)
            if review_text:
                reviews.append(review_text)
        
        return reviews
    
    def _extract_rating(self, soup):
        """æå–è©•åˆ†"""
        # Momo ç‰¹å®šé¸æ“‡å™¨
        momo_selectors = [
            'span.rating-score',
            'div[class*="rating"]',
            'span[class*="score"]',
            'div.star-score',
            'span[class*="mrate"]',        # Momo è©•åˆ†
            'div[data-testid*="rating"]'
        ]
        
        # é€šç”¨é¸æ“‡å™¨
        general_selectors = ['.rating', '[data-rating]', '.star', '.score', '.rate']
        
        all_selectors = momo_selectors + general_selectors
        
        for selector in all_selectors:
            try:
                elem = soup.select_one(selector)
                if elem:
                    text = elem.get_text(strip=True)
                    # æå–æ•¸å­—éƒ¨åˆ†
                    numbers = [c for c in text if c.isdigit() or c == '.']
                    if numbers:
                        rating_str = ''.join(numbers)
                        try:
                            rating = float(rating_str)
                            if 0 <= rating <= 5.0:  # ç¢ºä¿è©•åˆ†åœ¨æœ‰æ•ˆç¯„åœ
                                return rating
                        except ValueError:
                            continue
            except:
                continue
        
        return 0


def scrape_products(urls, is_dynamic=False):
    """
    æ‰¹æ¬¡çˆ¬å–å¤šå€‹å•†å“
    
    Args:
        urls: å•†å“é€£çµåˆ—è¡¨
        is_dynamic: æ˜¯å¦ç‚ºå‹•æ…‹é é¢
    
    Returns:
        list: å•†å“è³‡è¨Šåˆ—è¡¨
    """
    scraper = ProductScraper()
    products = []
    
    for i, url in enumerate(urls, 1):
        print(f"â³ æ­£åœ¨çˆ¬å–ç¬¬ {i}/{len(urls)} å€‹å•†å“...")
        product = scraper.extract_product_info(url, is_dynamic)
        
        if product:
            products.append(product)
            print(f"âœ… æˆåŠŸçˆ¬å–: {product['name'][:50]}")
        else:
            print(f"âš ï¸  çˆ¬å–å¤±æ•—: {url}")
        
        time.sleep(1)  # é¿å…éæ–¼é »ç¹çš„è«‹æ±‚
    
    return products
