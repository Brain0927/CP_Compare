"""
ç›¸ä¼¼å•†å“æœå°‹æ¨¡çµ„ - è‡ªå‹•æ‰¾å‡ºç›¸ä¼¼å•†å“
"""
import requests
from bs4 import BeautifulSoup
from config.settings import HEADERS, REQUEST_TIMEOUT
import re
from typing import List, Dict


class SimilarProductFinder:
    """å°‹æ‰¾ç›¸ä¼¼å•†å“"""
    
    def __init__(self):
        self.headers = HEADERS
        self.timeout = REQUEST_TIMEOUT
    
    def extract_product_info_from_url(self, url: str) -> Dict:
        """
        å¾URLæå–å•†å“è³‡è¨Š
        
        Returns:
            {
                'name': str,
                'price': float,
                'category': str,
                'specs': dict,
                'url': str
            }
        """
        try:
            # Momo å•†å“è‡ªå‹•ä½¿ç”¨å‹•æ…‹çˆ¬å–
            is_dynamic = 'momo.com.tw' in url.lower()
            
            if is_dynamic:
                from selenium import webdriver
                from selenium.webdriver.common.by import By
                from selenium.webdriver.support.ui import WebDriverWait
                from selenium.webdriver.support import expected_conditions as EC
                import time
                
                driver = None
                try:
                    options = webdriver.ChromeOptions()
                    options.add_argument('--no-sandbox')
                    options.add_argument('--disable-dev-shm-usage')
                    options.add_argument('--start-maximized')
                    
                    driver = webdriver.Chrome(options=options)
                    driver.get(url)
                    
                    # ç­‰å¾…åƒ¹æ ¼å…ƒç´ è¼‰å…¥
                    WebDriverWait(driver, 10).until(
                        EC.presence_of_all_elements_located((By.CLASS_NAME, "money"))
                    )
                    time.sleep(2)
                    
                    soup = BeautifulSoup(driver.page_source, 'html.parser')
                except:
                    # Selenium å¤±æ•—ï¼Œå›é€€åˆ°éœæ…‹çˆ¬å–
                    response = requests.get(url, headers=self.headers, timeout=self.timeout)
                    response.encoding = 'utf-8'
                    soup = BeautifulSoup(response.content, 'html.parser')
                finally:
                    if driver:
                        driver.quit()
            else:
                response = requests.get(url, headers=self.headers, timeout=self.timeout)
                response.encoding = 'utf-8'
                soup = BeautifulSoup(response.content, 'html.parser')
            
            # æå–åŸºæœ¬è³‡è¨Š
            product_info = {
                'name': self._extract_name(soup),
                'price': self._extract_price(soup),
                'category': self._extract_category(soup, url),
                'specs': self._extract_specs(soup),
                'url': url,
                'reviews': self._extract_reviews(soup),
                'rating': self._extract_rating(soup)
            }
            
            return product_info
            
        except Exception as e:
            print(f"âŒ æå–å¤±æ•—: {e}")
            return None
    
    def _extract_name(self, soup):
        """æå–å•†å“åç¨±"""
        # Momo ç‰¹å®šé¸æ“‡å™¨
        momo_selectors = [
            'h1.title',
            'h1[class*="title"]',
            'div.goods-name',
            'span.goods-title',
            'div[data-testid*="name"]'
        ]
        
        # é€šç”¨é¸æ“‡å™¨
        general_selectors = ['h1', '.product-title', '[data-name]', '.title', 'h2']
        
        all_selectors = momo_selectors + general_selectors
        
        for selector in all_selectors:
            try:
                elem = soup.select_one(selector)
                if elem:
                    name = elem.get_text(strip=True)
                    if name and len(name) > 3:  # ç¢ºä¿ä¸æ˜¯å¤ªçŸ­çš„æ–‡æœ¬
                        return name
            except:
                continue
        
        return "æœªçŸ¥å•†å“"
    
    def _extract_price(self, soup):
        """æå–åƒ¹æ ¼ - æ”¯æ´å¤šå€‹å¹³å°ï¼Œå„ªå…ˆæå–ä¿ƒéŠ·åƒ¹"""
        
        # === Momo ç‰¹å®šé‚è¼¯ï¼šå„ªå…ˆä½¿ç”¨ä¿ƒéŠ·åƒ¹ï¼Œå¦å‰‡ä½¿ç”¨å¸‚å”®åƒ¹ ===
        # ä¿ƒéŠ·åƒ¹ (ç´…è‰²é¡¯ç¤ºçš„åƒ¹æ ¼)
        promo_price = soup.find('span', class_='seoPrice')
        if promo_price:
            text = promo_price.get_text(strip=True)
            price_str = ''.join(c for c in text if c.isdigit() or c in '.,')
            price_str = price_str.replace(',', '')
            try:
                price = float(price_str)
                if price > 0:
                    return price
            except:
                pass
        
        # å¸‚å”®åƒ¹ (åˆªé™¤ç·šçš„åƒ¹æ ¼)
        sale_price = soup.find('del', class_='seoPrice')
        if sale_price:
            text = sale_price.get_text(strip=True)
            price_str = ''.join(c for c in text if c.isdigit() or c in '.,')
            price_str = price_str.replace(',', '')
            try:
                price = float(price_str)
                if price > 0:
                    return price
            except:
                pass
        
        # === å‚™ç”¨é¸æ“‡å™¨ï¼ˆå…¶ä»–å¹³å°æˆ–çµæ§‹ï¼‰ ===
        momo_selectors = [
            'span.money',                      # Momo ä¸»è¦åƒ¹æ ¼
            'p.current-price span.money',      # Momo å®Œæ•´è·¯å¾‘
            'span[class*="money"]',            # Momo æ¨¡ç³ŠåŒ¹é…
            'div.goods-price',
            'strong.price',
            'em.price',
            'span[class*="salesprice"]',
            'span[class*="sale-price"]',
        ]
        
        # é€šç”¨é¸æ“‡å™¨ï¼ˆå‚™ç”¨ï¼‰
        general_selectors = [
            '.price', '[data-price]', '.product-price', '.sale-price', 
            '.final-price', '.current-price', '.priceText'
        ]
        
        all_selectors = momo_selectors + general_selectors
        
        for selector in all_selectors:
            try:
                elem = soup.select_one(selector)
                if elem:
                    # å˜—è©¦å¾ data å±¬æ€§ä¸­ç²å–
                    if elem.get('data-price'):
                        try:
                            return float(elem.get('data-price'))
                        except:
                            pass
                    
                    # å¾æ–‡æœ¬å…§å®¹ä¸­æå–
                    text = elem.get_text(strip=True)
                    if text:
                        # ç§»é™¤éæ•¸å­—å­—å…ƒï¼ˆä¿ç•™å°æ•¸é»å’Œé€—è™Ÿï¼‰
                        price_str = ''.join(c for c in text if c.isdigit() or c in '.,')
                        
                        # ç§»é™¤é€—è™Ÿï¼ˆåƒä½åˆ†éš”ç¬¦ï¼‰
                        price_str = price_str.replace(',', '')
                        
                        # ç§»é™¤å¤šå€‹å°æ•¸é»ï¼Œåªä¿ç•™æœ€å¾Œä¸€å€‹
                        if price_str.count('.') > 1:
                            parts = price_str.split('.')
                            price_str = '.'.join([parts[0], parts[-1]])
                        
                        if price_str and price_str != '.':
                            try:
                                price = float(price_str)
                                if price > 0:  # ç¢ºä¿åƒ¹æ ¼æœ‰æ•ˆ
                                    return price
                            except ValueError:
                                continue
            except:
                continue
        
        return 0
    
    def _extract_category(self, soup, url: str) -> str:
        """æå–å•†å“é¡åˆ¥"""
        # å˜—è©¦å¾éºµåŒ…å±‘å°èˆªæå–
        breadcrumb = soup.select_one('.breadcrumb, .breadcrumbs')
        if breadcrumb:
            items = breadcrumb.find_all(['li', 'a'])
            if len(items) >= 2:
                return items[-2].get_text(strip=True)
        
        # å¾URLæå–
        if 'phone' in url or 'iphone' in url or 'samsung' in url:
            return 'æ‰‹æ©Ÿ'
        elif 'laptop' in url or 'notebook' in url or 'macbook' in url:
            return 'ç­†é›»'
        elif 'headphone' in url or 'earphone' in url or 'earbud' in url:
            return 'è€³æ©Ÿ'
        elif 'watch' in url:
            return 'æ™ºèƒ½æ‰‹éŒ¶'
        elif 'tablet' in url or 'ipad' in url:
            return 'å¹³æ¿'
        
        return 'é›»å­ç”¢å“'
    
    def _extract_specs(self, soup) -> dict:
        """æå–è¦æ ¼"""
        specs = {}
        
        # å°‹æ‰¾è¦æ ¼è¡¨
        spec_sections = soup.find_all(['dl', '.specs', '[data-specs]'])
        
        for section in spec_sections:
            dts = section.find_all('dt')
            dds = section.find_all('dd')
            
            for dt, dd in zip(dts, dds):
                key = dt.get_text(strip=True)
                value = dd.get_text(strip=True)
                if key and value:
                    specs[key] = value
        
        return specs
    
    def _extract_reviews(self, soup) -> list:
        """æå–è©•è«–"""
        reviews = []
        review_elements = soup.find_all(['div', 'li'], class_=lambda x: x and 'review' in x.lower())
        
        for elem in review_elements[:5]:
            review_text = elem.get_text(strip=True)
            if review_text:
                reviews.append(review_text)
        
        return reviews
    
    def _extract_rating(self, soup) -> float:
        """æå–è©•åˆ†"""
        # Momo ç‰¹å®šé¸æ“‡å™¨
        momo_selectors = [
            'span.rating-score',
            'div[class*="rating"]',
            'span[class*="score"]',
            'div.star-score',
            'span[class*="mrate"]',        # Momo è©•åˆ†
            'div[data-testid*="rating"]'
        ]
        
        # é€šç”¨é¸æ“‡å™¨
        general_selectors = ['.rating', '[data-rating]', '.star', '.score', '.rate']
        
        all_selectors = momo_selectors + general_selectors
        
        for selector in all_selectors:
            try:
                elem = soup.select_one(selector)
                if elem:
                    text = elem.get_text(strip=True)
                    # æå–æ•¸å­—éƒ¨åˆ†
                    numbers = [c for c in text if c.isdigit() or c == '.']
                    if numbers:
                        rating_str = ''.join(numbers)
                        try:
                            rating = float(rating_str)
                            if 0 <= rating <= 5.0:  # ç¢ºä¿è©•åˆ†åœ¨æœ‰æ•ˆç¯„åœ
                                return rating
                        except ValueError:
                            continue
            except:
                continue
        
        return 0
    
    def generate_search_queries(self, product_info: Dict) -> List[str]:
        """
        ç”Ÿæˆæœå°‹æŸ¥è©¢
        
        åŸºæ–¼å•†å“åç¨±ã€é¡åˆ¥ç”Ÿæˆæœå°‹é—œéµå­—
        """
        queries = []
        
        # åŸºæ–¼å“ç‰Œ
        name_parts = product_info['name'].split()
        if name_parts:
            brand = name_parts[0]
            queries.append(brand)
        
        # åŸºæ–¼é¡åˆ¥
        category = product_info.get('category', 'é›»å­ç”¢å“')
        if category != 'é›»å­ç”¢å“':
            queries.append(category)
        
        # åŸºæ–¼å®Œæ•´åç¨±
        queries.append(product_info['name'])
        
        # åŸºæ–¼è¦æ ¼
        for spec_name in product_info['specs'].keys():
            queries.append(spec_name)
        
        return list(set(queries))  # ç§»é™¤é‡è¤‡
    
    def build_search_urls(self, product_info: Dict, platform: str = 'momo') -> List[str]:
        """
        å»ºç«‹æœå°‹URL
        
        æ ¹æ“šä¸åŒå¹³å°æ§‹å»ºæœå°‹é€£çµ
        """
        queries = self.generate_search_queries(product_info)
        search_urls = []
        
        if platform == 'momo':
            base_url = "https://www.momoshop.com.tw/search/searchShop.php?keyword="
            for query in queries[:3]:
                search_urls.append(base_url + query)
        
        elif platform == 'pchome':
            base_url = "https://www.pchome.com.tw/search/?q="
            for query in queries[:3]:
                search_urls.append(base_url + query)
        
        elif platform == 'shopee':
            base_url = "https://shopee.tw/search?keyword="
            for query in queries[:3]:
                search_urls.append(base_url + query)
        
        return search_urls
    
    def find_similar_products_on_same_platform(self, product_url: str, max_results: int = 3) -> List[Dict]:
        """
        åœ¨åŒä¸€å¹³å°ä¸Šå°‹æ‰¾ç›¸ä¼¼å•†å“
        
        Returns:
            List[Dict]: åŒ…å«åŸå•†å“å’Œç›¸ä¼¼å•†å“çš„åˆ—è¡¨
        """
        # æå–åŸå•†å“è³‡è¨Š
        product_info = self.extract_product_info_from_url(product_url)
        
        if not product_info:
            return []
        
        # åˆ¤æ–·å¹³å°
        if 'momo.com.tw' in product_url:
            platform = 'momo'
        elif 'pchome' in product_url:
            platform = 'pchome'
        elif 'shopee' in product_url:
            platform = 'shopee'
        else:
            platform = 'momo'  # é è¨­
        
        print(f"\n{'='*60}")
        print(f"ğŸ” å°‹æ‰¾ç›¸ä¼¼å•†å“...")
        print(f"{'='*60}")
        print(f"ğŸ·ï¸  åŸå•†å“: {product_info['name']}")
        print(f"ğŸ’° åŸåƒ¹æ ¼: ${product_info['price']:,.0f}")
        print(f"ğŸ“‚ é¡åˆ¥: {product_info['category']}")
        print(f"â­ è©•åˆ†: {product_info['rating']:.1f}/5.0")
        
        # ç”Ÿæˆæœå°‹é—œéµå­—
        search_queries = self.generate_search_queries(product_info)
        
        print(f"ğŸ” æœå°‹é—œéµå­—: {', '.join(search_queries[:3])}")
        print(f"{'='*60}\n")
        
        similar_products = [product_info]  # åŒ…å«åŸå•†å“
        
        # ç‚ºæ¯å€‹æœå°‹é—œéµå­—å»ºç«‹æœå°‹ URL
        search_urls = self.build_search_urls(product_info, platform)
        
        print(f"ğŸ“Œ æ¨è–¦æœå°‹æ–¹å¼:")
        print(f"   1. è¨ªå• {platform.upper()} å¹³å°")
        print(f"   2. æœå°‹ä»¥ä¸‹é—œéµå­—:")
        for i, query in enumerate(search_queries[:3], 1):
            print(f"      {i}. {query}")
        print(f"   3. ç¯©é¸ç›¸ä¼¼è¦æ ¼çš„å•†å“é€²è¡Œæ¯”è¼ƒ\n")
        
        return similar_products


def get_similar_products(product_url: str) -> List[Dict]:
    """
    ä¸»å‡½æ•¸ï¼šå–å¾—ç›¸ä¼¼å•†å“
    
    Args:
        product_url: å•†å“é€£çµ
    
    Returns:
        list: ç›¸ä¼¼å•†å“åˆ—è¡¨
    """
    finder = SimilarProductFinder()
    similar_products = finder.find_similar_products_on_same_platform(product_url, max_results=3)
    
    return similar_products
